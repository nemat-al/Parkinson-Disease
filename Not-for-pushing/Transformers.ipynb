{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qW5qq0Ku5do"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(2)\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "#tf.enable_eager_execution()\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Conv1D\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras import backend as K\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_pos_2(input,nb):\n",
        "    input_pos_encoding = tf.constant(nb, shape=[input.shape[1]], dtype=\"int32\")/input.shape[1]\n",
        "    input_pos_encoding = tf.cast(tf.reshape(input_pos_encoding, [1,10]),tf.float32)\n",
        "    input = tf.add(input ,input_pos_encoding)\n",
        "    return input\n",
        "\n",
        "def stack_block_transformer(num_transformer_blocks):\n",
        "    input1 = keras.Input(shape=(100, 1))\n",
        "    x = input1\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x,100,2)\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    x = layers.Dense(10, activation='selu')(x)\n",
        "    return input1,x\n",
        "\n",
        "def stack_block_transformer_spatial(num_transformer_blocks,x):\n",
        "  for _ in range(num_transformer_blocks):\n",
        "      x = transformer_encoder(x,10*18,2)\n",
        "  x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def transformer_encoder(inputs,key_dim,num_heads):\n",
        "    dropout=0.1\n",
        "    # Normalization and Attention\n",
        "    print(\"transformer_encoder\",inputs.shape)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=key_dim, num_heads=num_heads\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Dense(key_dim, activation='softmax')(x)\n",
        "    return x + res\n",
        "\n",
        "\n",
        "def multiple_transformer(nb):\n",
        "    '''\n",
        "\n",
        "    :param nb: number of features ( indicates the number of parallel branches)\n",
        "    :return:\n",
        "    '''\n",
        "    # initialise with the first input\n",
        "\n",
        "    num_transformer_blocks = 2  #hyperparameter\n",
        "    input_, transformer_ = stack_block_transformer(num_transformer_blocks)\n",
        "    transformers = []\n",
        "    inputs = []\n",
        "    transformers.append(transformer_)\n",
        "    inputs.append(input_)\n",
        "    for i in range(1,nb ):\n",
        "        input_i, transformer_i = stack_block_transformer(num_transformer_blocks)\n",
        "        inputs.append(input_i) \n",
        "        transformer_i = add_pos_2(transformer_i,i)\n",
        "        transformers.append(transformer_i)\n",
        "  \n",
        "    x = layers.concatenate(transformers, axis=-1)\n",
        "    x = tf.expand_dims(x, -1) #-1 denotes the last dimension\n",
        "    x = stack_block_transformer_spatial(num_transformer_blocks,x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = layers.Dense(100, activation='selu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = layers.Dense(20, activation='selu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    answer = layers.Dense(1, activation='sigmoid')(x)\n",
        "  \n",
        "    model = Model(inputs, answer)\n",
        "    opt = optimizers.RMSprop(lr=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'],experimental_run_tf_function=False)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "\n",
        "def multiple_transformer_5_level(nb):\n",
        "    '''\n",
        "    Model for severity prediction , 5 classes output\n",
        "    :param nb:  number of parallel branch\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "  # initialise with the first input\n",
        "\n",
        "    num_transformer_blocks = 2  #hyperparameter\n",
        "    input_, transformer_ = stack_block_transformer(num_transformer_blocks)\n",
        "    transformers = []\n",
        "    inputs = []\n",
        "    transformers.append(transformer_)\n",
        "    inputs.append(input_)\n",
        "    for i in range(1,nb ):\n",
        "        input_i, transformer_i = stack_block_transformer(num_transformer_blocks)\n",
        "        inputs.append(input_i)\n",
        "        transformer_i = add_pos_2(transformer_i,i)\n",
        "        transformers.append(transformer_i)\n",
        "  \n",
        "    x = layers.concatenate(transformers, axis=-1)\n",
        "    x = tf.expand_dims(x, -1) #-1 denotes the last dimension\n",
        "    x = stack_block_transformer_spatial(num_transformer_blocks,x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = layers.Dense(100, activation='selu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = layers.Dense(20, activation='selu')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    answer = layers.Dense(5, activation='sigmoid')(x)\n",
        "  \n",
        "    model = Model(inputs, answer)\n",
        "    opt = optimizers.RMSprop(lr=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'],experimental_run_tf_function=False)\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "eiEcIyMVvOzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "7jETCljMvmt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('/content/drive/MyDrive/Thesis/3rd_semester/Data/original')\n",
        "np.random.seed(2)\n",
        "\n",
        "\n",
        "\n",
        "class Data:\n",
        "\n",
        "    def __init__(self,  input_data,  deep, gait_cycle, step=50, features=np.arange(1, 19), pk_level = True):\n",
        "        '''\n",
        "\n",
        "        :param load_or_get:  1: load data , 0: load preloaded datas ( npy)\n",
        "        :param deep:  data in the format for deep learning algorithms\n",
        "        :param gait_cycle: number of gait cycle per signal\n",
        "        :param step: overlap between gait signals\n",
        "        :param features: signals to be loaded ( coming from sensors)\n",
        "        :param pk_level: if true , y is the parkinson level according\n",
        "        '''\n",
        "\n",
        "        self.deep = deep\n",
        "        self.step = step\n",
        "        self.nb_gait_cycle = gait_cycle\n",
        "\n",
        "\n",
        "        self.features_to_load = features\n",
        "        self.nb_features = self.features_to_load.shape[0]\n",
        "        ###############\n",
        "        self.X_data = np.array([])  # np.ones((self.nb_gait_cycle,self.nb_features))\n",
        "        self.y_data = np.array([])\n",
        "        self.nb_data_per_person = np.array([0])\n",
        "\n",
        "\n",
        "        files = sorted(glob.glob(os.path.join(input_data, '*txt')))\n",
        "        self.ctrl_list = []\n",
        "        self.pk_list = []\n",
        "        for file in files:\n",
        "\n",
        "            if file.find(\".txt\") != -1:  # if control (\"01.txt\")\n",
        "                if file.find(\"Co\") != -1:  # if control\n",
        "                    self.ctrl_list.append(file)\n",
        "                elif file.find(\"Pt\") != -1:  # if control\n",
        "                    if pk_level:\n",
        "                        if  file.find('SiPt02')!= -1:\n",
        "                            pass\n",
        "                        elif file.find('SiPt07')!= -1:\n",
        "                            pass\n",
        "                        else:\n",
        "                            self.pk_list.append(file)\n",
        "                    else :\n",
        "                        self.pk_list.append(file)\n",
        "\n",
        "        random.shuffle(self.ctrl_list)\n",
        "        random.shuffle(self.pk_list)\n",
        "        self.pk_level = pk_level\n",
        "        if pk_level == True:\n",
        "            self.levels = pd.read_csv( os.path.join(input_data, \"demographics.csv\"))\n",
        "            self.levels.set_index('ID', inplace=True)\n",
        "        self.load(norm=None)\n",
        "     \n",
        "    def add_pos(self,input):\n",
        "       # Positional encoding\n",
        "        input_pos_encoding = tf.range(input.shape[1])/input.shape[1]\n",
        "        input_pos_encoding = tf.expand_dims(input_pos_encoding, -1)\n",
        "        input_pos_encoding= tf.cast(tf.tile(input_pos_encoding, [1,input.shape[2]]),tf.float32)\n",
        "        # Add the positional encoding\n",
        "        input = input + input_pos_encoding\n",
        "        return input\n",
        "\n",
        "\n",
        "    def separate_fold(self, fold_number, total_fold=10):\n",
        "        '''\n",
        "\n",
        "        :param fold_number: Fold number\n",
        "        :param total_fold: Total number of fols\n",
        "        :return:\n",
        "        '''\n",
        "        proportion = 1 / total_fold  # .10 for 10 folds\n",
        "        X = [self.X_ctrl, self.X_park]\n",
        "        y = [self.y_ctrl, self.y_park]\n",
        "        patients = [self.nb_data_per_person[:self.last_ctrl_patient], self.nb_data_per_person[self.last_ctrl_patient:]] # counts separated by classe\n",
        "        patients[1]= patients[1] - patients[1][0]\n",
        "        diff_count = np.diff(self.nb_data_per_person)\n",
        "        diff_count = [diff_count[:self.last_ctrl_patient], diff_count[self.last_ctrl_patient:]]\n",
        "        self.count_val = np.array([0])\n",
        "        self.count_train = np.array([0])\n",
        "        for i in range(len(X)):\n",
        "            nbr_patients =  int(len(patients[i]) *proportion)\n",
        "            start_patient = int(fold_number*nbr_patients )\n",
        "            end_patient = (fold_number+1)*nbr_patients\n",
        "            id_start = patients[i][start_patient]  # segment start\n",
        "            id_end = patients[i][end_patient]  # end segment\n",
        "            if i ==0 :\n",
        "                self.X_val = X[i][id_start:id_end,:,:]\n",
        "                self.X_train = np.delete(X[i], np.arange(id_start,id_end) , 0)\n",
        "\n",
        "                self.y_val = y[i][id_start:id_end]\n",
        "                self.y_train = np.delete(y[i], np.arange(id_start,id_end) , 0)\n",
        "\n",
        "\n",
        "                self.count_val = np.append(self.count_val, diff_count[i][start_patient: end_patient])\n",
        "                self.count_train = np.append(self.count_train, np.delete(diff_count[i], np.arange(start_patient, end_patient)))\n",
        "\n",
        "\n",
        "\n",
        "            else:\n",
        "                start_patient = start_patient #+ patients[0].shape[0]  # patients0.shape 0 is the number of patients in the first class\n",
        "                end_patient =  end_patient# +patients[0].shape[0]\n",
        "                self.X_val = np.vstack((self.X_val, X[i][id_start:id_end,:,:]))\n",
        "                self.X_train = np.vstack((self.X_train, np.delete(X[i], np.arange(id_start,id_end) , 0) ))\n",
        "\n",
        "                self.y_val = np.vstack((self.y_val, y[i][id_start:id_end] ))\n",
        "                self.y_train = np.vstack((self.y_train, np.delete(y[i], np.arange(id_start,id_end) , 0) ))\n",
        "\n",
        "                self.count_val = np.append( self.count_val , diff_count[i][start_patient: end_patient])\n",
        "                self.count_train = np.append(self.count_train,np.delete(diff_count[i], np.arange(start_patient, end_patient)) )\n",
        "\n",
        "        self.count_val = np.cumsum(self.count_val)\n",
        "        self.count_train = np.cumsum(self.count_train )\n",
        "        self.X_val = layers.LayerNormalization(epsilon=1e-6)(self.X_val)\n",
        "        self.X_train = layers.LayerNormalization(epsilon=1e-6)(self.X_train)\n",
        "        self.X_val = self.add_pos(self.X_val)\n",
        "        self.X_train = self.add_pos(self.X_train)\n",
        "      \n",
        "\n",
        "    def load(self, norm = 'std'):\n",
        "        print(\"load training control \")\n",
        "        self.load_data(self.ctrl_list, 0)\n",
        "        if self.deep == 1:\n",
        "            self.last_ctrl= self.X_data.shape[2]\n",
        "            self.last_ctrl_patient = len(self.nb_data_per_person)\n",
        "        print(\"load training parkinson \")\n",
        "\n",
        "\n",
        "        self.load_data(self.pk_list, 1)  # ncycle, nfeature, nombre de data\n",
        "\n",
        "\n",
        "        ## all datas are loaded at this point, preprocessing now\n",
        "        if self.deep == 1:\n",
        "            self.X_data = self.X_data.transpose(2,0 , 1)  #0, 1\n",
        "\n",
        "            if norm == 'std ':\n",
        "                self.normalize()\n",
        "            elif norm == 'l2':\n",
        "                self.X_data = self.normalize_l2(self.X_data)\n",
        "\n",
        "        if self.pk_level:\n",
        "            self.one_hot_encoding()\n",
        "\n",
        "        if self.deep == 1:\n",
        "            self.X_ctrl = self.X_data[:self.last_ctrl]\n",
        "            self.y_ctrl =  self.y_data[:self.last_ctrl]\n",
        "            self.X_park = self.X_data[self.last_ctrl:]\n",
        "            self.y_park = self.y_data[self.last_ctrl:]\n",
        "\n",
        "        print(\"saving training \")\n",
        "        np.save(\"Xdata\", self.X_data)\n",
        "        np.save(\"ydata\", self.y_data)\n",
        "        np.save('data_person',self.nb_data_per_person)\n",
        "        np.save('ctrl_list', self.ctrl_list)\n",
        "        np.save('pk_list', self.pk_list)\n",
        "\n",
        "    def normalize(self):\n",
        "        '''\n",
        "\n",
        "        :return: Normalize to have a mean =  and std =1\n",
        "        '''\n",
        "        mean_train = np.mean(self.X_data,(0,1))\n",
        "        std_train = np.std(self.X_data,(0,1))\n",
        "        self.X_data= abs((self.X_data - mean_train) / std_train)\n",
        "        #self.X_test= (self.X_test - mean_train) / std_train\n",
        "    def one_hot_encoding(self):\n",
        "        '''\n",
        "\n",
        "        :return: return one hot encoding vector for severity prediction\n",
        "        '''\n",
        "        self.y_data[self.y_data<=4]=0\n",
        "        self.y_data[(4<self.y_data) & (self.y_data <15)]=1\n",
        "        self.y_data[(15<= self.y_data) & (self.y_data<25)]=2\n",
        "        self.y_data[(25<= self.y_data) & (self.y_data<35)] = 3\n",
        "        self.y_data[35<= self.y_data] = 4\n",
        "        np.set_printoptions(threshold=sys.maxsize)\n",
        "        # self.y_data = np.squeeze(self.y_data)\n",
        "        #self.y_data = self.y_data[~np.isnan(self.y_data)] # Line 2\n",
        "        self.y_data = np.nan_to_num(self.y_data)\n",
        "        print(self.y_data)\n",
        "        print(self.y_data.size)\n",
        "        self.y_data = to_categorical(self.y_data)\n",
        "\n",
        "    def normalize_l2(self, data):\n",
        "        '''\n",
        "\n",
        "        :param data:  Function to perform L2 normalization\n",
        "        :return:\n",
        "        '''\n",
        "        data = keras.backend.l2_normalize(data, axis=(1, 2))\n",
        "        data = tf.keras.backend.get_value(data)\n",
        "        return data\n",
        "\n",
        "\n",
        "    def load_data(self, liste, y):\n",
        "        '''\n",
        "\n",
        "        :param liste: list of patients filepaths\n",
        "        :param y: 0 for control, 1 for parkinson\n",
        "        :return:\n",
        "        '''\n",
        "\n",
        "        for i in range(0, len(liste)):\n",
        "            datas = np.loadtxt(liste[i])  # num cycle, n features\n",
        "            datas = datas[:, self.features_to_load]\n",
        "            print(datas.shape[0])\n",
        "            if  self.pk_level :\n",
        "                print(\"1\")\n",
        "                y =self.find_level(liste[i])\n",
        "\n",
        "            if self.deep == 1:\n",
        "                print(\"2\")\n",
        "                X_data, y_data , self.nb_data_per_person = self.generate_datas(datas, y, self.nb_data_per_person)\n",
        "              \n",
        "            else:\n",
        "                print(\"3\")\n",
        "                X_data, y_data = self.generate_datas_ml(datas, y)\n",
        "            if (self.X_data).size == 0:\n",
        "                print(\"4\")\n",
        "                self.X_data = X_data\n",
        "                self.y_data = y_data\n",
        "            else:\n",
        "                print(\"5\")\n",
        "                if self.deep == 1:\n",
        "                    print(\"6\")\n",
        "                    self.X_data = np.dstack((self.X_data, X_data))\n",
        "                else:\n",
        "                    print(\"7\")\n",
        "                    self.X_data = np.vstack((self.X_data, X_data))  # shape nb data --- vector size\n",
        "                self.y_data = np.vstack((self.y_data, y_data))\n",
        "\n",
        "            print(X_data.shape, self.X_data.shape,flush=True)\n",
        "           \n",
        "\n",
        "\n",
        "    def find_level(self,file):\n",
        "        '''\n",
        "\n",
        "        :param file: Dataframe\n",
        "        :return:\n",
        "        '''\n",
        "        start = '/content/drive/MyDrive/Thesis/3rd_semester/Data/original/'\n",
        "        end = '_'\n",
        "        id = (file.split(start))[1].split(end)[0]\n",
        "        y = self.levels.loc[id,'UPDRS']\n",
        "        return y\n",
        "\n",
        "\n",
        "    def generate_datas(self, datas, y, data_list):\n",
        "        '''\n",
        "        :param datas:  datas loaded for 1 patient\n",
        "        :param y: label of the patient\n",
        "        :param data_list: list containing the number of segments per patients\n",
        "        :return:\n",
        "        '''\n",
        "        count = 0\n",
        "        X_data = np.array([])\n",
        "        y_data = np.array([])\n",
        "        nb_datas = int(datas.shape[0] - self.nb_gait_cycle)\n",
        "        for start in range(0, nb_datas, self.step):\n",
        "            end = start + self.nb_gait_cycle\n",
        "            data = datas[start:end, :]\n",
        "            if X_data.size == 0:\n",
        "                X_data = data\n",
        "                y_data = y\n",
        "            else:\n",
        "                if (self.deep == 1):\n",
        "                    X_data = np.dstack((X_data, data))\n",
        "                else:\n",
        "                    X_data = np.vstack((X_data, data))\n",
        "                y_data = np.vstack((y_data, y))\n",
        "            count = count + 1\n",
        "        data_list = np.append(data_list, count+ data_list[-1])\n",
        "        return X_data, y_data, data_list\n",
        "\n",
        "\n",
        "    def get_datas(self):\n",
        "        return self.X_data, self.y_data, self.X_test, self.y_test, self.X_val, self.y_val"
      ],
      "metadata": {
        "id": "_Tq9QXkIvuzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix,  classification_report, accuracy_score\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "class Results:\n",
        "    def __init__(self, filename_seg, filename_patient):\n",
        "        '''\n",
        "\n",
        "        :param filename_seg:  Filename  (.csv) where to save results at the segment levels\n",
        "        :param filename_patient: Filename  (.csv) where to save results at the patient levels\n",
        "        '''\n",
        "        self.results_patients = np.zeros(3)\n",
        "        self.results_segments = np.zeros(3)\n",
        "        self.filename_seg = filename_seg\n",
        "        self.filename_patient = filename_patient\n",
        "    def add_result( self,res, accuracy,  segments = True  ):\n",
        "        '''\n",
        "\n",
        "        :param res: result of classification report (sklearn )\n",
        "        :param accuracy:\n",
        "        :param segments: 1 to add results at the segment level\n",
        "        :return:\n",
        "        '''\n",
        "        if segments:\n",
        "            specificity = res['0.0']['recall']\n",
        "            sensitivy =  res['1.0']['recall']\n",
        "        else:\n",
        "            specificity = res['0']['recall']\n",
        "            sensitivy =  res['1']['recall']\n",
        "        all = np.array([specificity, sensitivy, accuracy])\n",
        "\n",
        "        if segments:\n",
        "            self.results_segments = np.vstack((self.results_segments, all))\n",
        "        else:\n",
        "            self.results_patients = np.vstack((self.results_patients, all ))\n",
        "\n",
        "    def validate_patient(self, model, x_val, y_val, count):\n",
        "        '''\n",
        "\n",
        "        :param model: trained model after 1 fold of cross validation\n",
        "        :param x_val: x_Val for 1 forld of cross validation\n",
        "        :param y_val: y_Val for 1 forld of cross validation\n",
        "        :param count: vector containing the number of segments per patient\n",
        "        :return:  save the results of the fold\n",
        "\n",
        "        '''\n",
        "        ## per segments\n",
        "        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n",
        "        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True )\n",
        "        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg))\n",
        "        self.add_result(res, acc,True  )\n",
        "\n",
        "        eval = []\n",
        "        y = []\n",
        "        pred = []\n",
        "        for m in range(1, len(count)):\n",
        "            i = count[m]\n",
        "            j = count[m - 1]\n",
        "            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2), y_val[j:i])\n",
        "            eval.append(score)\n",
        "            y.append(np.int(np.mean(y_val[j:i])))\n",
        "            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2)))\n",
        "            pred.append(np.mean(p))\n",
        "\n",
        "        res = classification_report(y, np.rint(pred), output_dict = True )\n",
        "        print(classification_report(y, np.rint(pred)))\n",
        "\n",
        "        acc = accuracy_score(np.rint(y), np.rint(pred))\n",
        "        self.add_result(res, acc, False )\n",
        "\n",
        "        #np.savetxt(self.filename_patient, self.results_patients, delimiter=\",\")\n",
        "        #np.savetxt(self.filename_seg, self.results_segments, delimiter=\",\")\n",
        "        res_segments_dict = {'Specificity': self.results_segments[1:,0],'Sensitivity': self.results_segments[1:,1],'Accuracy': self.results_segments[1:,2]  }\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Specificity': self.results_patients[1:,0],'Sensitivity': self.results_patients[1:,1],'Accuracy': self.results_patients[1:,2]  }\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "\n",
        "\n",
        "\n",
        "class Results_level:\n",
        "    '''\n",
        "    Class to save results for severity prediction\n",
        "    '''\n",
        "    def __init__(self, filename_seg, filename_patient, dir):\n",
        "        '''\n",
        "\n",
        "        :param filename_seg: filename (csv) where to save the results\n",
        "        :param filename_patient:\n",
        "        :param dir: directory where results files are saved\n",
        "        '''\n",
        "        self.results_patients = np.zeros(1)\n",
        "        self.results_segments = np.zeros(1)\n",
        "        self.filename_seg = filename_seg\n",
        "        self.filename_patient = filename_patient\n",
        "        self.gt = np.array([])\n",
        "        self.pred = np.array([])\n",
        "        self.dir = dir\n",
        "    def add_result( self,res, accuracy,  segments = True  ):\n",
        "\n",
        "        all = np.array([ accuracy])\n",
        "\n",
        "        if segments:\n",
        "            self.results_segments = np.vstack((self.results_segments, all))\n",
        "        else:\n",
        "            self.results_patients = np.vstack((self.results_patients, all ))\n",
        "\n",
        "\n",
        "\n",
        "    def validate_patient(self, model, x_val, y_val, count):\n",
        "        '''\n",
        "\n",
        "        :param model: trained model after 1 fold of cross validation\n",
        "        :param x_val: x_Val for 1 forld of cross validation\n",
        "        :param y_val: y_Val for 1 forld of cross validation\n",
        "        :param count: vector containing the number of segments per patient\n",
        "        :return:  save the results of the fold\n",
        "\n",
        "        '''\n",
        "        ## per segments\n",
        "        pred_seg = model.predict(np.split(x_val, x_val.shape[2], axis=2))\n",
        "        res = classification_report(np.rint(y_val), np.rint(pred_seg), output_dict = True )\n",
        "        acc = accuracy_score(np.rint(y_val), np.rint(pred_seg))\n",
        "        self.add_result(res, acc,True  )\n",
        "\n",
        "        eval = []\n",
        "        y = []\n",
        "        pred = []\n",
        "        for m in range(1, len(count)):\n",
        "            i = count[m]\n",
        "            j = count[m - 1]\n",
        "            score = model.evaluate(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2), y_val[j:i])\n",
        "            eval.append(score)\n",
        "            y_gt = np.argmax(y_val[j:i],1)\n",
        "            y_gt , _ = stats.mode(y_gt, axis = None)\n",
        "            y.append(y_gt[0])\n",
        "            p = np.rint(model.predict(np.split(x_val[j:i, :, :], x_val.shape[2], axis=2)))\n",
        "            p = np.argmax(p, 1 )\n",
        "            p, _ = stats.mode(p, axis=None)\n",
        "            pred.append(p[0])\n",
        "\n",
        "        res = classification_report(y, np.rint(pred), output_dict = True )\n",
        "        print(classification_report(y, np.rint(pred)))\n",
        "        self.gt = np.append(self.gt, y)\n",
        "        self.pred = np.append(self.pred, np.rint(pred))\n",
        "        acc = accuracy_score(np.rint(y), np.rint(pred))\n",
        "        self.add_result(res, acc, False )\n",
        "        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "        print(res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def write_results(self):\n",
        "        '''\n",
        "        Called at the end to write the final result files\n",
        "        :return:\n",
        "        '''\n",
        "        res_segments_dict = {'Accuracy': self.results_segments[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_segments_dict)\n",
        "        df.to_csv(self.filename_seg)\n",
        "        res_patients_dict =  {'Accuracy': self.results_patients[1:,0]}\n",
        "        df = pd.DataFrame.from_dict(res_patients_dict)\n",
        "        df.to_csv(self.filename_patient)\n",
        "        file_pred = os.path.join(self.dir, 'pred.csv')\n",
        "        file_gt = os.path.join(self.dir, 'gt.csv')\n",
        "        np.savetxt(file_pred, self.pred, delimiter=\",\" )\n",
        "        np.savetxt(file_gt,self.gt, delimiter=\",\")\n",
        "        res = classification_report(self.gt, self.pred)\n",
        "        print(res)\n",
        "        self.cm = confusion_matrix(self.gt, self.pred)\n",
        "        file_conf_matrx = os.path.join(self.dir, 'confusion_matrix.csv')\n",
        "        np.savetxt(file_conf_matrx, self.cm, delimiter=\",\")"
      ],
      "metadata": {
        "id": "EfWC0K7wwR7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(2) #2\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "#from src.data_utils2 import Datas\n",
        "#from src.results import Results,Results_level\n",
        "\n",
        "#from src.algo import multiple_cnn1D, multiple_cnn1D5_level\n",
        "#from src.data_utils import Data\n",
        "\n",
        "def train( model, datas, lr, log_filename, filename):\n",
        "    \"\"\"\n",
        "\n",
        "    :param model: Initial untrained model\n",
        "    :param datas:  data object\n",
        "    :param lr: learning rate\n",
        "    :param log_filename: filename where the training results will be saved ( for each epoch)\n",
        "    :param filename: file where the weights will be saved\n",
        "    :return:  trained model\n",
        "    \"\"\"\n",
        "    X_train = datas.X_train\n",
        "    y_train = datas.y_train\n",
        "    X_val = datas.X_val\n",
        "    y_val = datas.y_val\n",
        "    \n",
        "    \n",
        "    logger = CSVLogger(log_filename, separator=',', append=True)\n",
        "    for i in (np.arange(1,4)*5):  # 10-20    1-10\n",
        "\n",
        "        checkpointer = ModelCheckpoint(filepath=filename , monitor='val_accuracy', verbose=1, save_best_only=True,save_freq='epoch')\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, verbose=0, mode='auto')\n",
        "\n",
        "        callbacks_list = [checkpointer, early_stopping, logger]\n",
        "        print(\"Y_train\")\n",
        "        print(len(y_train))\n",
        "        history = model.fit(np.split(X_train,X_train.shape[2], axis=2), \\\n",
        "                            # history  = model.fit(X_data,\\\n",
        "                            y_train, \\\n",
        "                            verbose=1, \\\n",
        "                            shuffle=True, \\\n",
        "                            epochs= 100,\\\n",
        "                            batch_size=110, \\\n",
        "                            # validation_data=(X_val, y_val),\\\n",
        "                            validation_data=(np.split(X_val, X_val.shape[2], axis=2), y_val), \\\n",
        "                            callbacks=callbacks_list)\n",
        "\n",
        "        model.load_weights(filename)\n",
        "        lr =  lr / 2\n",
        "        rms = optimizers.Nadam(lr=lr)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=rms, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "def train_classifier(args):\n",
        "    '''\n",
        "    Function that performs the detection of Parkinson\n",
        "    :param args: Input arguments\n",
        "    :return:\n",
        "    '''\n",
        "    exp_name = args.exp_name\n",
        "    subfolder = os.path.join(args.output, exp_name +'_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n",
        "        \"%H_%M\"))\n",
        "    file_result_patients = os.path.join(subfolder,'res_pat.csv')\n",
        "    file_result_segments = os.path.join(subfolder,'res_seg.csv')\n",
        "    model_file = os.path.join(subfolder, \"model.json\")\n",
        "    if not os.path.exists(subfolder):\n",
        "        os.makedirs(subfolder)\n",
        "\n",
        "    val_results = Results(file_result_segments, file_result_patients)\n",
        "    datas = Data(args.input_data, 1, 100, pk_level= False )\n",
        "\n",
        "    for i in range(0, 10):\n",
        "        lr = 0.001\n",
        "        model = multiple_transformer(datas.X_data.shape[2])\n",
        "        model_json = model.to_json()\n",
        "        with open(model_file, \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "\n",
        "        print('fold', str(i))\n",
        "        datas.separate_fold(i)\n",
        "        log_filename = os.path.join( subfolder ,\"training_\" + str(i) + \".csv\")\n",
        "        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".hdf5\")\n",
        "        model = train(model, datas, lr, log_filename, w_filename)\n",
        "        print('Validation !!')\n",
        "        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n",
        "\n",
        "def train_severity(args):\n",
        "    '''\n",
        "\n",
        "    :param args: Input arguments\n",
        "    :return:\n",
        "    '''\n",
        "    features = np.arange(1, 19)\n",
        "\n",
        "\n",
        "    exp_name = args.exp_name\n",
        "\n",
        "    subfolder = os.path.join(args.output, exp_name +'_' + datetime.datetime.now().strftime(\"%m_%d\"), datetime.datetime.now().strftime(\n",
        "        \"%H_%M\"))\n",
        "    if not os.path.exists(subfolder):\n",
        "        os.makedirs(subfolder)\n",
        "    file_result_patients = os.path.join(subfolder ,'res_pat.csv')\n",
        "    file_result_segments = os.path.join(subfolder ,'res_seg.csv')\n",
        "\n",
        "    model_file = os.path.join(subfolder, \"model.json\")\n",
        "\n",
        "    val_results = Results_level(file_result_segments, file_result_patients, subfolder )\n",
        "    datas = Data(args.input_data, 1, 100)  # modif\n",
        "    lr = 0.001\n",
        "    for i in range(0,10):\n",
        "\n",
        "        model = multiple_transformer_5_level(datas.X_data.shape[2])\n",
        "        model_json = model.to_json()\n",
        "        with open(model_file, \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        print('fold', str(i))\n",
        "        datas.separate_fold(i)\n",
        "        log_filename = os.path.join(subfolder, \"trainig\" + str(i) + \".csv\")\n",
        "        w_filename = os.path.join(subfolder ,\"weights_\" + str(i) + \".hdf5\")\n",
        "        model = train(model, datas, lr, log_filename,  w_filename )\n",
        "        print('Validation !!')\n",
        "        val_results.validate_patient(model, datas.X_val, datas.y_val, datas.count_val)\n",
        "        val_results.write_results()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-input_data\", default='/content/drive/MyDrive/Thesis/3rd_semester/Data/original', type=str)\n",
        "    #' \n",
        "    parser.add_argument(\"-exp_name\", default='train_severity', type=str, help = 'train_classifier ; train_severity')\n",
        "    parser.add_argument(\"-output\", default='output', type=str)\n",
        "    args = parser.parse_args(args=[])\n",
        "    if not os.path.exists(args.output):\n",
        "        os.makedirs(args.output)\n",
        "    if args.exp_name == 'train_classifier' :\n",
        "        train_classifier(args)\n",
        "    if args.exp_name == 'train_severity':\n",
        "        train_severity(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOwFAGe4wXRT",
        "outputId": "8dddabc9-a5e1-4369-ee26-4eead0edc84c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [2.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [3.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]\n",
            " [4.]]\n",
            "65433\n",
            "saving training \n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 180, 1)\n",
            "transformer_encoder (None, 180, 180)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_15 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_16 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_17 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_18 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 100, 1)      2           ['input_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 100, 1)      2           ['input_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 100, 1)      2           ['input_4[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 100, 1)      2           ['input_5[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_20 (LayerN  (None, 100, 1)      2           ['input_6[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_24 (LayerN  (None, 100, 1)      2           ['input_7[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_28 (LayerN  (None, 100, 1)      2           ['input_8[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_32 (LayerN  (None, 100, 1)      2           ['input_9[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_36 (LayerN  (None, 100, 1)      2           ['input_10[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_40 (LayerN  (None, 100, 1)      2           ['input_11[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_44 (LayerN  (None, 100, 1)      2           ['input_12[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_48 (LayerN  (None, 100, 1)      2           ['input_13[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_52 (LayerN  (None, 100, 1)      2           ['input_14[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_56 (LayerN  (None, 100, 1)      2           ['input_15[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_60 (LayerN  (None, 100, 1)      2           ['input_16[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_64 (LayerN  (None, 100, 1)      2           ['input_17[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_68 (LayerN  (None, 100, 1)      2           ['input_18[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 100, 1)      2           ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 100, 1)      1401        ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 100, 1)      1401        ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 100, 1)      1401        ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, 100, 1)      1401        ['layer_normalization_16[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, 100, 1)      1401        ['layer_normalization_20[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_12 (Multi  (None, 100, 1)      1401        ['layer_normalization_24[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_14 (Multi  (None, 100, 1)      1401        ['layer_normalization_28[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_16 (Multi  (None, 100, 1)      1401        ['layer_normalization_32[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_18 (Multi  (None, 100, 1)      1401        ['layer_normalization_36[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_20 (Multi  (None, 100, 1)      1401        ['layer_normalization_40[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_22 (Multi  (None, 100, 1)      1401        ['layer_normalization_44[0][0]', \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " HeadAttention)                                                   'layer_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_24 (Multi  (None, 100, 1)      1401        ['layer_normalization_48[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_26 (Multi  (None, 100, 1)      1401        ['layer_normalization_52[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_28 (Multi  (None, 100, 1)      1401        ['layer_normalization_56[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_30 (Multi  (None, 100, 1)      1401        ['layer_normalization_60[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_60[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, 100, 1)      1401        ['layer_normalization_64[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 100, 1)      1401        ['layer_normalization_68[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 100, 1)      1401        ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 100, 1)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 100, 1)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 100, 1)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 100, 1)       0           ['multi_head_attention_8[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_10[0][0]']\n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_12[0][0]']\n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_14[0][0]']\n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_16[0][0]']\n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_18[0][0]']\n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_20[0][0]']\n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_22[0][0]']\n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_24[0][0]']\n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_26[0][0]']\n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_28[0][0]']\n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_30[0][0]']\n",
            "                                                                                                  \n",
            " dropout_32 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_32[0][0]']\n",
            "                                                                                                  \n",
            " dropout_34 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_34[0][0]']\n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 100, 1)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 100, 1)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 100, 1)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 100, 1)      0           ['dropout_6[0][0]',              \n",
            " ambda)                                                           'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_16 (TFOpL  (None, 100, 1)      0           ['dropout_8[0][0]',              \n",
            " ambda)                                                           'input_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_20 (TFOpL  (None, 100, 1)      0           ['dropout_10[0][0]',             \n",
            " ambda)                                                           'input_6[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_24 (TFOpL  (None, 100, 1)      0           ['dropout_12[0][0]',             \n",
            " ambda)                                                           'input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_28 (TFOpL  (None, 100, 1)      0           ['dropout_14[0][0]',             \n",
            " ambda)                                                           'input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_32 (TFOpL  (None, 100, 1)      0           ['dropout_16[0][0]',             \n",
            " ambda)                                                           'input_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_36 (TFOpL  (None, 100, 1)      0           ['dropout_18[0][0]',             \n",
            " ambda)                                                           'input_10[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_40 (TFOpL  (None, 100, 1)      0           ['dropout_20[0][0]',             \n",
            " ambda)                                                           'input_11[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_44 (TFOpL  (None, 100, 1)      0           ['dropout_22[0][0]',             \n",
            " ambda)                                                           'input_12[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_48 (TFOpL  (None, 100, 1)      0           ['dropout_24[0][0]',             \n",
            " ambda)                                                           'input_13[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_52 (TFOpL  (None, 100, 1)      0           ['dropout_26[0][0]',             \n",
            " ambda)                                                           'input_14[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_56 (TFOpL  (None, 100, 1)      0           ['dropout_28[0][0]',             \n",
            " ambda)                                                           'input_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_60 (TFOpL  (None, 100, 1)      0           ['dropout_30[0][0]',             \n",
            " ambda)                                                           'input_16[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_64 (TFOpL  (None, 100, 1)      0           ['dropout_32[0][0]',             \n",
            " ambda)                                                           'input_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_68 (TFOpL  (None, 100, 1)      0           ['dropout_34[0][0]',             \n",
            " ambda)                                                           'input_18[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 100, 1)      0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 100, 1)      2           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 100, 1)      2           ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_16[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_21 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_20[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_25 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_24[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_29 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_28[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_33 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_32[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_37 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_36[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_41 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_40[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_45 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_44[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_49 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_48[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_53 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_52[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_57 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_56[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_61 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_60[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_65 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_64[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_69 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_68[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 100, 1)      2           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 100, 100)     200         ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 100, 100)     200         ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 100, 100)     200         ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 100, 100)     200         ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 100, 100)     200         ['layer_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 100, 100)     200         ['layer_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 100, 100)     200         ['layer_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 100, 100)     200         ['layer_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " dense_27 (Dense)               (None, 100, 100)     200         ['layer_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " dense_30 (Dense)               (None, 100, 100)     200         ['layer_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " dense_33 (Dense)               (None, 100, 100)     200         ['layer_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " dense_36 (Dense)               (None, 100, 100)     200         ['layer_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " dense_39 (Dense)               (None, 100, 100)     200         ['layer_normalization_53[0][0]'] \n",
            "                                                                                                  \n",
            " dense_42 (Dense)               (None, 100, 100)     200         ['layer_normalization_57[0][0]'] \n",
            "                                                                                                  \n",
            " dense_45 (Dense)               (None, 100, 100)     200         ['layer_normalization_61[0][0]'] \n",
            "                                                                                                  \n",
            " dense_48 (Dense)               (None, 100, 100)     200         ['layer_normalization_65[0][0]'] \n",
            "                                                                                                  \n",
            " dense_51 (Dense)               (None, 100, 100)     200         ['layer_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100, 100)     200         ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 100, 100)    0           ['dense_3[0][0]',                \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 100, 100)    0           ['dense_6[0][0]',                \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 100, 100)    0           ['dense_9[0][0]',                \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_17 (TFOpL  (None, 100, 100)    0           ['dense_12[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_16[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_21 (TFOpL  (None, 100, 100)    0           ['dense_15[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_20[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_25 (TFOpL  (None, 100, 100)    0           ['dense_18[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_24[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_29 (TFOpL  (None, 100, 100)    0           ['dense_21[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_28[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_33 (TFOpL  (None, 100, 100)    0           ['dense_24[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_32[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_37 (TFOpL  (None, 100, 100)    0           ['dense_27[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_36[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_41 (TFOpL  (None, 100, 100)    0           ['dense_30[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_40[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_45 (TFOpL  (None, 100, 100)    0           ['dense_33[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_44[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_49 (TFOpL  (None, 100, 100)    0           ['dense_36[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_48[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_53 (TFOpL  (None, 100, 100)    0           ['dense_39[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_52[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_57 (TFOpL  (None, 100, 100)    0           ['dense_42[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_56[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_61 (TFOpL  (None, 100, 100)    0           ['dense_45[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_60[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_65 (TFOpL  (None, 100, 100)    0           ['dense_48[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_64[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_69 (TFOpL  (None, 100, 100)    0           ['dense_51[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_68[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 100, 100)    0           ['dense[0][0]',                  \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 100, 100)    200         ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_17[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_22 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_21[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_26 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_25[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_30 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_29[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_34 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_33[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_38 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_37[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_42 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_41[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_46 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_45[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_50 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_49[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_54 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_53[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_58 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_57[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_62 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_61[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_66 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_65[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_70 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_69[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 100, 100)    200         ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 100, 100)    80700       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 100, 100)    80700       ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 100, 100)    80700       ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, 100, 100)    80700       ['layer_normalization_18[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, 100, 100)    80700       ['layer_normalization_22[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_13 (Multi  (None, 100, 100)    80700       ['layer_normalization_26[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_15 (Multi  (None, 100, 100)    80700       ['layer_normalization_30[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_17 (Multi  (None, 100, 100)    80700       ['layer_normalization_34[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_19 (Multi  (None, 100, 100)    80700       ['layer_normalization_38[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_21 (Multi  (None, 100, 100)    80700       ['layer_normalization_42[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_23 (Multi  (None, 100, 100)    80700       ['layer_normalization_46[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_25 (Multi  (None, 100, 100)    80700       ['layer_normalization_50[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_27 (Multi  (None, 100, 100)    80700       ['layer_normalization_54[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_54[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_29 (Multi  (None, 100, 100)    80700       ['layer_normalization_58[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_31 (Multi  (None, 100, 100)    80700       ['layer_normalization_62[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_62[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 100, 100)    80700       ['layer_normalization_66[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 100, 100)    80700       ['layer_normalization_70[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 100, 100)    80700       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 100, 100)     0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 100, 100)     0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 100, 100)     0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 100, 100)     0           ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_11[0][0]']\n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_13[0][0]']\n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_15[0][0]']\n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_17[0][0]']\n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_19[0][0]']\n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_21[0][0]']\n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_23[0][0]']\n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_25[0][0]']\n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_27[0][0]']\n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_29[0][0]']\n",
            "                                                                                                  \n",
            " dropout_31 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_31[0][0]']\n",
            "                                                                                                  \n",
            " dropout_33 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_33[0][0]']\n",
            "                                                                                                  \n",
            " dropout_35 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_35[0][0]']\n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 100, 100)     0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 100, 100)    0           ['dropout_3[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 100, 100)    0           ['dropout_5[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 100, 100)    0           ['dropout_7[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_18 (TFOpL  (None, 100, 100)    0           ['dropout_9[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_17[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_22 (TFOpL  (None, 100, 100)    0           ['dropout_11[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_21[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_26 (TFOpL  (None, 100, 100)    0           ['dropout_13[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_25[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_30 (TFOpL  (None, 100, 100)    0           ['dropout_15[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_29[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_34 (TFOpL  (None, 100, 100)    0           ['dropout_17[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_33[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_38 (TFOpL  (None, 100, 100)    0           ['dropout_19[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_37[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_42 (TFOpL  (None, 100, 100)    0           ['dropout_21[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_41[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_46 (TFOpL  (None, 100, 100)    0           ['dropout_23[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_45[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_50 (TFOpL  (None, 100, 100)    0           ['dropout_25[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_49[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_54 (TFOpL  (None, 100, 100)    0           ['dropout_27[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_53[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_58 (TFOpL  (None, 100, 100)    0           ['dropout_29[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_57[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_62 (TFOpL  (None, 100, 100)    0           ['dropout_31[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_61[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_66 (TFOpL  (None, 100, 100)    0           ['dropout_33[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_65[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_70 (TFOpL  (None, 100, 100)    0           ['dropout_35[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_69[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 100, 100)    0           ['dropout_1[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 100, 100)    200         ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_19 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_18[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_23 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_22[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_27 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_26[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_31 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_30[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_35 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_34[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_39 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_38[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_43 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_42[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_47 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_46[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_51 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_50[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_55 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_54[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_59 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_58[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_63 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_62[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_67 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_66[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_71 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_70[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 100, 100)    200         ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 100, 100)     10100       ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 100, 100)     10100       ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 100, 100)     10100       ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 100, 100)     10100       ['layer_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 100, 100)     10100       ['layer_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 100, 100)     10100       ['layer_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 100, 100)     10100       ['layer_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 100, 100)     10100       ['layer_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " dense_28 (Dense)               (None, 100, 100)     10100       ['layer_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " dense_31 (Dense)               (None, 100, 100)     10100       ['layer_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 100, 100)     10100       ['layer_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " dense_37 (Dense)               (None, 100, 100)     10100       ['layer_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " dense_40 (Dense)               (None, 100, 100)     10100       ['layer_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " dense_43 (Dense)               (None, 100, 100)     10100       ['layer_normalization_59[0][0]'] \n",
            "                                                                                                  \n",
            " dense_46 (Dense)               (None, 100, 100)     10100       ['layer_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " dense_49 (Dense)               (None, 100, 100)     10100       ['layer_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " dense_52 (Dense)               (None, 100, 100)     10100       ['layer_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 100)     10100       ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 100, 100)    0           ['dense_4[0][0]',                \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 100, 100)    0           ['dense_7[0][0]',                \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 100, 100)    0           ['dense_10[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_19 (TFOpL  (None, 100, 100)    0           ['dense_13[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_18[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_23 (TFOpL  (None, 100, 100)    0           ['dense_16[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_22[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_27 (TFOpL  (None, 100, 100)    0           ['dense_19[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_26[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_31 (TFOpL  (None, 100, 100)    0           ['dense_22[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_30[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_35 (TFOpL  (None, 100, 100)    0           ['dense_25[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_34[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_39 (TFOpL  (None, 100, 100)    0           ['dense_28[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_38[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_43 (TFOpL  (None, 100, 100)    0           ['dense_31[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_42[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_47 (TFOpL  (None, 100, 100)    0           ['dense_34[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_46[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_51 (TFOpL  (None, 100, 100)    0           ['dense_37[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_50[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_55 (TFOpL  (None, 100, 100)    0           ['dense_40[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_54[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_59 (TFOpL  (None, 100, 100)    0           ['dense_43[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_58[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_63 (TFOpL  (None, 100, 100)    0           ['dense_46[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_62[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_67 (TFOpL  (None, 100, 100)    0           ['dense_49[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_66[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_71 (TFOpL  (None, 100, 100)    0           ['dense_52[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_70[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 100, 100)    0           ['dense_1[0][0]',                \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 100)         0           ['tf.__operators__.add_7[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_2 (Gl  (None, 100)         0           ['tf.__operators__.add_11[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_3 (Gl  (None, 100)         0           ['tf.__operators__.add_15[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_4 (Gl  (None, 100)         0           ['tf.__operators__.add_19[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_5 (Gl  (None, 100)         0           ['tf.__operators__.add_23[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_6 (Gl  (None, 100)         0           ['tf.__operators__.add_27[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_7 (Gl  (None, 100)         0           ['tf.__operators__.add_31[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_8 (Gl  (None, 100)         0           ['tf.__operators__.add_35[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_9 (Gl  (None, 100)         0           ['tf.__operators__.add_39[0][0]']\n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling1d_10 (G  (None, 100)         0           ['tf.__operators__.add_43[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_11 (G  (None, 100)         0           ['tf.__operators__.add_47[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_12 (G  (None, 100)         0           ['tf.__operators__.add_51[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_13 (G  (None, 100)         0           ['tf.__operators__.add_55[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_14 (G  (None, 100)         0           ['tf.__operators__.add_59[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_15 (G  (None, 100)         0           ['tf.__operators__.add_63[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_16 (G  (None, 100)         0           ['tf.__operators__.add_67[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_17 (G  (None, 100)         0           ['tf.__operators__.add_71[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 100)         0           ['tf.__operators__.add_3[0][0]'] \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 10)           1010        ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 10)           1010        ['global_average_pooling1d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 10)           1010        ['global_average_pooling1d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 10)           1010        ['global_average_pooling1d_4[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 10)           1010        ['global_average_pooling1d_5[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 10)           1010        ['global_average_pooling1d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 10)           1010        ['global_average_pooling1d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_26 (Dense)               (None, 10)           1010        ['global_average_pooling1d_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_29 (Dense)               (None, 10)           1010        ['global_average_pooling1d_9[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_32 (Dense)               (None, 10)           1010        ['global_average_pooling1d_10[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 10)           1010        ['global_average_pooling1d_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_38 (Dense)               (None, 10)           1010        ['global_average_pooling1d_12[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_41 (Dense)               (None, 10)           1010        ['global_average_pooling1d_13[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_44 (Dense)               (None, 10)           1010        ['global_average_pooling1d_14[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_47 (Dense)               (None, 10)           1010        ['global_average_pooling1d_15[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_50 (Dense)               (None, 10)           1010        ['global_average_pooling1d_16[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_53 (Dense)               (None, 10)           1010        ['global_average_pooling1d_17[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           1010        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.math.add (TFOpLambda)       (None, 10)           0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.add_1 (TFOpLambda)     (None, 10)           0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.add_2 (TFOpLambda)     (None, 10)           0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_3 (TFOpLambda)     (None, 10)           0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_4 (TFOpLambda)     (None, 10)           0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_5 (TFOpLambda)     (None, 10)           0           ['dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_6 (TFOpLambda)     (None, 10)           0           ['dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_7 (TFOpLambda)     (None, 10)           0           ['dense_26[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_8 (TFOpLambda)     (None, 10)           0           ['dense_29[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_9 (TFOpLambda)     (None, 10)           0           ['dense_32[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_10 (TFOpLambda)    (None, 10)           0           ['dense_35[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_11 (TFOpLambda)    (None, 10)           0           ['dense_38[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_12 (TFOpLambda)    (None, 10)           0           ['dense_41[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_13 (TFOpLambda)    (None, 10)           0           ['dense_44[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_14 (TFOpLambda)    (None, 10)           0           ['dense_47[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_15 (TFOpLambda)    (None, 10)           0           ['dense_50[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_16 (TFOpLambda)    (None, 10)           0           ['dense_53[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 180)          0           ['dense_2[0][0]',                \n",
            "                                                                  'tf.math.add[0][0]',            \n",
            "                                                                  'tf.math.add_1[0][0]',          \n",
            "                                                                  'tf.math.add_2[0][0]',          \n",
            "                                                                  'tf.math.add_3[0][0]',          \n",
            "                                                                  'tf.math.add_4[0][0]',          \n",
            "                                                                  'tf.math.add_5[0][0]',          \n",
            "                                                                  'tf.math.add_6[0][0]',          \n",
            "                                                                  'tf.math.add_7[0][0]',          \n",
            "                                                                  'tf.math.add_8[0][0]',          \n",
            "                                                                  'tf.math.add_9[0][0]',          \n",
            "                                                                  'tf.math.add_10[0][0]',         \n",
            "                                                                  'tf.math.add_11[0][0]',         \n",
            "                                                                  'tf.math.add_12[0][0]',         \n",
            "                                                                  'tf.math.add_13[0][0]',         \n",
            "                                                                  'tf.math.add_14[0][0]',         \n",
            "                                                                  'tf.math.add_15[0][0]',         \n",
            "                                                                  'tf.math.add_16[0][0]']         \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda)    (None, 180, 1)       0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_72 (LayerN  (None, 180, 1)      2           ['tf.expand_dims[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_36 (Multi  (None, 180, 1)      2521        ['layer_normalization_72[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_36 (Dropout)           (None, 180, 1)       0           ['multi_head_attention_36[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_72 (TFOpL  (None, 180, 1)      0           ['dropout_36[0][0]',             \n",
            " ambda)                                                           'tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " layer_normalization_73 (LayerN  (None, 180, 1)      2           ['tf.__operators__.add_72[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_54 (Dense)               (None, 180, 180)     360         ['layer_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_73 (TFOpL  (None, 180, 180)    0           ['dense_54[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_72[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_74 (LayerN  (None, 180, 180)    360         ['tf.__operators__.add_73[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 180, 180)    260460      ['layer_normalization_74[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_37 (Dropout)           (None, 180, 180)     0           ['multi_head_attention_37[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_74 (TFOpL  (None, 180, 180)    0           ['dropout_37[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_73[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_75 (LayerN  (None, 180, 180)    360         ['tf.__operators__.add_74[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_55 (Dense)               (None, 180, 180)     32580       ['layer_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_75 (TFOpL  (None, 180, 180)    0           ['dense_55[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_74[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_18 (G  (None, 180)         0           ['tf.__operators__.add_75[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " dropout_38 (Dropout)           (None, 180)          0           ['global_average_pooling1d_18[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_56 (Dense)               (None, 100)          18100       ['dropout_38[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_39 (Dropout)           (None, 100)          0           ['dense_56[0][0]']               \n",
            "                                                                                                  \n",
            " dense_57 (Dense)               (None, 20)           2020        ['dropout_39[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_40 (Dropout)           (None, 20)           0           ['dense_57[0][0]']               \n",
            "                                                                                                  \n",
            " dense_58 (Dense)               (None, 5)            105         ['dropout_40[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,005,540\n",
            "Trainable params: 2,005,540\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "fold 0\n",
            "Y_train\n",
            "58600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.4130 - accuracy: 0.4766\n",
            "Epoch 1: val_accuracy improved from -inf to 0.36880, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 695s 1s/step - loss: 0.4130 - accuracy: 0.4766 - val_loss: 0.4917 - val_accuracy: 0.3688\n",
            "Epoch 2/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.6323\n",
            "Epoch 2: val_accuracy improved from 0.36880 to 0.42500, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 682s 1s/step - loss: 0.3237 - accuracy: 0.6323 - val_loss: 0.4697 - val_accuracy: 0.4250\n",
            "Epoch 3/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.7267\n",
            "Epoch 3: val_accuracy improved from 0.42500 to 0.45573, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 675s 1s/step - loss: 0.2613 - accuracy: 0.7267 - val_loss: 0.4995 - val_accuracy: 0.4557\n",
            "Epoch 4/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.7830\n",
            "Epoch 4: val_accuracy improved from 0.45573 to 0.57295, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 663s 1s/step - loss: 0.2182 - accuracy: 0.7830 - val_loss: 0.4381 - val_accuracy: 0.5730\n",
            "Epoch 5/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.8189\n",
            "Epoch 5: val_accuracy did not improve from 0.57295\n",
            "533/533 [==============================] - 653s 1s/step - loss: 0.1888 - accuracy: 0.8189 - val_loss: 0.4793 - val_accuracy: 0.5348\n",
            "Epoch 6/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.8454\n",
            "Epoch 6: val_accuracy improved from 0.57295 to 0.57603, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 652s 1s/step - loss: 0.1668 - accuracy: 0.8454 - val_loss: 0.4333 - val_accuracy: 0.5760\n",
            "Epoch 7/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.8615\n",
            "Epoch 7: val_accuracy did not improve from 0.57603\n",
            "533/533 [==============================] - 651s 1s/step - loss: 0.1521 - accuracy: 0.8615 - val_loss: 0.4960 - val_accuracy: 0.5459\n",
            "Epoch 8/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.8746\n",
            "Epoch 8: val_accuracy improved from 0.57603 to 0.58247, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 655s 1s/step - loss: 0.1388 - accuracy: 0.8746 - val_loss: 0.4747 - val_accuracy: 0.5825\n",
            "Epoch 9/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.8848\n",
            "Epoch 9: val_accuracy did not improve from 0.58247\n",
            "533/533 [==============================] - 653s 1s/step - loss: 0.1293 - accuracy: 0.8848 - val_loss: 0.4848 - val_accuracy: 0.5582\n",
            "Epoch 10/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.8916\n",
            "Epoch 10: val_accuracy improved from 0.58247 to 0.58496, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 655s 1s/step - loss: 0.1225 - accuracy: 0.8916 - val_loss: 0.4767 - val_accuracy: 0.5850\n",
            "Epoch 11/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.8976\n",
            "Epoch 11: val_accuracy improved from 0.58496 to 0.59110, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 652s 1s/step - loss: 0.1144 - accuracy: 0.8976 - val_loss: 0.4878 - val_accuracy: 0.5911\n",
            "Epoch 12/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9034\n",
            "Epoch 12: val_accuracy improved from 0.59110 to 0.59886, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 653s 1s/step - loss: 0.1102 - accuracy: 0.9034 - val_loss: 0.4659 - val_accuracy: 0.5989\n",
            "Epoch 13/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.1035 - accuracy: 0.9091\n",
            "Epoch 13: val_accuracy did not improve from 0.59886\n",
            "533/533 [==============================] - 657s 1s/step - loss: 0.1035 - accuracy: 0.9091 - val_loss: 0.5313 - val_accuracy: 0.5551\n",
            "Epoch 14/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9116\n",
            "Epoch 14: val_accuracy improved from 0.59886 to 0.62242, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 657s 1s/step - loss: 0.0999 - accuracy: 0.9116 - val_loss: 0.4805 - val_accuracy: 0.6224\n",
            "Epoch 15/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9149\n",
            "Epoch 15: val_accuracy did not improve from 0.62242\n",
            "533/533 [==============================] - 654s 1s/step - loss: 0.0966 - accuracy: 0.9149 - val_loss: 0.4850 - val_accuracy: 0.6170\n",
            "Epoch 16/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9196\n",
            "Epoch 16: val_accuracy did not improve from 0.62242\n",
            "533/533 [==============================] - 652s 1s/step - loss: 0.0928 - accuracy: 0.9196 - val_loss: 0.4862 - val_accuracy: 0.6078\n",
            "Epoch 17/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0901 - accuracy: 0.9208\n",
            "Epoch 17: val_accuracy did not improve from 0.62242\n",
            "533/533 [==============================] - 651s 1s/step - loss: 0.0901 - accuracy: 0.9208 - val_loss: 0.5280 - val_accuracy: 0.5850\n",
            "Epoch 18/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9253\n",
            "Epoch 18: val_accuracy improved from 0.62242 to 0.62608, saving model to output/train_severity_12_28/08_38/weights_0.hdf5\n",
            "533/533 [==============================] - 662s 1s/step - loss: 0.0863 - accuracy: 0.9253 - val_loss: 0.4803 - val_accuracy: 0.6261\n",
            "Epoch 19/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9245\n",
            "Epoch 19: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 659s 1s/step - loss: 0.0853 - accuracy: 0.9245 - val_loss: 0.5319 - val_accuracy: 0.5980\n",
            "Epoch 20/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9264\n",
            "Epoch 20: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 654s 1s/step - loss: 0.0826 - accuracy: 0.9264 - val_loss: 0.5389 - val_accuracy: 0.5691\n",
            "Epoch 21/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9293\n",
            "Epoch 21: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 650s 1s/step - loss: 0.0804 - accuracy: 0.9293 - val_loss: 0.5611 - val_accuracy: 0.6076\n",
            "Epoch 22/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9301\n",
            "Epoch 22: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 646s 1s/step - loss: 0.0792 - accuracy: 0.9301 - val_loss: 0.5343 - val_accuracy: 0.6107\n",
            "Epoch 23/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9320\n",
            "Epoch 23: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 653s 1s/step - loss: 0.0774 - accuracy: 0.9320 - val_loss: 0.5189 - val_accuracy: 0.5917\n",
            "Epoch 24/100\n",
            "533/533 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9337\n",
            "Epoch 24: val_accuracy did not improve from 0.62608\n",
            "533/533 [==============================] - 649s 1s/step - loss: 0.0754 - accuracy: 0.9337 - val_loss: 0.5014 - val_accuracy: 0.6068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/nadam.py:78: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Nadam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation !!\n",
            "214/214 [==============================] - 56s 264ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 273ms/step - loss: 0.1960 - accuracy: 0.7925\n",
            "5/5 [==============================] - 1s 263ms/step\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 1.6282 - accuracy: 0.1703\n",
            "8/8 [==============================] - 2s 261ms/step\n",
            "6/6 [==============================] - 2s 269ms/step - loss: 0.4946 - accuracy: 0.4817\n",
            "6/6 [==============================] - 2s 265ms/step\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 1.1658 - accuracy: 0.0954\n",
            "8/8 [==============================] - 2s 259ms/step\n",
            "8/8 [==============================] - 2s 269ms/step - loss: 0.0091 - accuracy: 0.9917\n",
            "8/8 [==============================] - 2s 263ms/step\n",
            "6/6 [==============================] - 2s 270ms/step - loss: 0.0304 - accuracy: 0.9896\n",
            "6/6 [==============================] - 2s 263ms/step\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.0794 - accuracy: 0.9378\n",
            "8/8 [==============================] - 2s 264ms/step\n",
            "8/8 [==============================] - 2s 270ms/step - loss: 0.3295 - accuracy: 0.6224\n",
            "8/8 [==============================] - 2s 261ms/step\n",
            "8/8 [==============================] - 2s 312ms/step - loss: 0.4278 - accuracy: 0.8035\n",
            "8/8 [==============================] - 4s 427ms/step\n",
            "8/8 [==============================] - 3s 342ms/step - loss: 0.0173 - accuracy: 1.0000\n",
            "8/8 [==============================] - 2s 271ms/step\n",
            "4/4 [==============================] - 1s 278ms/step - loss: 0.2727 - accuracy: 0.7143\n",
            "4/4 [==============================] - 1s 260ms/step\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.2193 - accuracy: 0.8091\n",
            "8/8 [==============================] - 2s 258ms/step\n",
            "6/6 [==============================] - 2s 278ms/step - loss: 0.1518 - accuracy: 0.8706\n",
            "6/6 [==============================] - 2s 272ms/step\n",
            "8/8 [==============================] - 2s 271ms/step - loss: 0.6787 - accuracy: 0.3112\n",
            "8/8 [==============================] - 2s 262ms/step\n",
            "3/3 [==============================] - 1s 270ms/step - loss: 0.2378 - accuracy: 0.8421\n",
            "3/3 [==============================] - 1s 272ms/step\n",
            "8/8 [==============================] - 2s 270ms/step - loss: 0.0696 - accuracy: 0.9502\n",
            "8/8 [==============================] - 2s 266ms/step\n",
            "8/8 [==============================] - 2s 277ms/step - loss: 0.5370 - accuracy: 0.4066\n",
            "8/8 [==============================] - 2s 263ms/step\n",
            "8/8 [==============================] - 2s 276ms/step - loss: 0.1136 - accuracy: 0.9046\n",
            "8/8 [==============================] - 2s 262ms/step\n",
            "4/4 [==============================] - 1s 264ms/step - loss: 0.0223 - accuracy: 0.9917\n",
            "4/4 [==============================] - 1s 263ms/step\n",
            "8/8 [==============================] - 2s 280ms/step - loss: 0.0102 - accuracy: 0.9876\n",
            "8/8 [==============================] - 2s 273ms/step\n",
            "17/17 [==============================] - 5s 271ms/step - loss: 0.8549 - accuracy: 0.4240\n",
            "17/17 [==============================] - 5s 266ms/step\n",
            "11/11 [==============================] - 3s 275ms/step - loss: 1.1356 - accuracy: 0.1503\n",
            "11/11 [==============================] - 3s 269ms/step\n",
            "8/8 [==============================] - 2s 277ms/step - loss: 0.2874 - accuracy: 0.6929\n",
            "8/8 [==============================] - 2s 265ms/step\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.6683 - accuracy: 0.0290\n",
            "8/8 [==============================] - 2s 258ms/step\n",
            "10/10 [==============================] - 3s 274ms/step - loss: 0.0912 - accuracy: 0.9331\n",
            "10/10 [==============================] - 3s 264ms/step\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 0.0048 - accuracy: 0.9959\n",
            "8/8 [==============================] - 2s 261ms/step\n",
            "6/6 [==============================] - 2s 268ms/step - loss: 1.9937 - accuracy: 0.0057\n",
            "6/6 [==============================] - 2s 263ms/step\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 0.5664 - accuracy: 0.7095\n",
            "8/8 [==============================] - 2s 265ms/step\n",
            "8/8 [==============================] - 2s 273ms/step - loss: 1.0236 - accuracy: 0.0041\n",
            "8/8 [==============================] - 2s 265ms/step\n",
            "6/6 [==============================] - 2s 276ms/step - loss: 0.0760 - accuracy: 0.9524\n",
            "6/6 [==============================] - 2s 261ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.78      0.74         9\n",
            "           2       0.50      1.00      0.67         2\n",
            "           3       0.50      0.22      0.31         9\n",
            "           4       0.75      0.90      0.82        10\n",
            "\n",
            "    accuracy                           0.67        30\n",
            "   macro avg       0.61      0.72      0.63        30\n",
            "weighted avg       0.64      0.67      0.63        30\n",
            "\n",
            "{'0': {'precision': 0.7, 'recall': 0.7777777777777778, 'f1-score': 0.7368421052631577, 'support': 9}, '2': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2}, '3': {'precision': 0.5, 'recall': 0.2222222222222222, 'f1-score': 0.30769230769230765, 'support': 9}, '4': {'precision': 0.75, 'recall': 0.9, 'f1-score': 0.8181818181818182, 'support': 10}, 'accuracy': 0.6666666666666666, 'macro avg': {'precision': 0.6125, 'recall': 0.725, 'f1-score': 0.6323457244509876, 'support': 30}, 'weighted avg': {'precision': 0.6433333333333333, 'recall': 0.6666666666666666, 'f1-score': 0.6305320410583568, 'support': 30}}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.70      0.78      0.74         9\n",
            "         2.0       0.50      1.00      0.67         2\n",
            "         3.0       0.50      0.22      0.31         9\n",
            "         4.0       0.75      0.90      0.82        10\n",
            "\n",
            "    accuracy                           0.67        30\n",
            "   macro avg       0.61      0.72      0.63        30\n",
            "weighted avg       0.64      0.67      0.63        30\n",
            "\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 100, 1)\n",
            "transformer_encoder (None, 100, 100)\n",
            "transformer_encoder (None, 180, 1)\n",
            "transformer_encoder (None, 180, 180)\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_20 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_21 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_25 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_26 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_27 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_29 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_31 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_33 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_34 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_35 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_19 (InputLayer)          [(None, 100, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " layer_normalization_82 (LayerN  (None, 100, 1)      2           ['input_20[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_86 (LayerN  (None, 100, 1)      2           ['input_21[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_90 (LayerN  (None, 100, 1)      2           ['input_22[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_94 (LayerN  (None, 100, 1)      2           ['input_23[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_98 (LayerN  (None, 100, 1)      2           ['input_24[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_102 (Layer  (None, 100, 1)      2           ['input_25[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_106 (Layer  (None, 100, 1)      2           ['input_26[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_110 (Layer  (None, 100, 1)      2           ['input_27[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_114 (Layer  (None, 100, 1)      2           ['input_28[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_118 (Layer  (None, 100, 1)      2           ['input_29[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_122 (Layer  (None, 100, 1)      2           ['input_30[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_126 (Layer  (None, 100, 1)      2           ['input_31[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_130 (Layer  (None, 100, 1)      2           ['input_32[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_134 (Layer  (None, 100, 1)      2           ['input_33[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_138 (Layer  (None, 100, 1)      2           ['input_34[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_142 (Layer  (None, 100, 1)      2           ['input_35[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_146 (Layer  (None, 100, 1)      2           ['input_36[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_78 (LayerN  (None, 100, 1)      2           ['input_19[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 100, 1)      1401        ['layer_normalization_82[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (Multi  (None, 100, 1)      1401        ['layer_normalization_86[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_86[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_44 (Multi  (None, 100, 1)      1401        ['layer_normalization_90[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_90[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_46 (Multi  (None, 100, 1)      1401        ['layer_normalization_94[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_94[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_48 (Multi  (None, 100, 1)      1401        ['layer_normalization_98[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_98[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_50 (Multi  (None, 100, 1)      1401        ['layer_normalization_102[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_102[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_52 (Multi  (None, 100, 1)      1401        ['layer_normalization_106[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_106[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_54 (Multi  (None, 100, 1)      1401        ['layer_normalization_110[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_110[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_56 (Multi  (None, 100, 1)      1401        ['layer_normalization_114[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_114[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_58 (Multi  (None, 100, 1)      1401        ['layer_normalization_118[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_118[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_60 (Multi  (None, 100, 1)      1401        ['layer_normalization_122[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_122[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_62 (Multi  (None, 100, 1)      1401        ['layer_normalization_126[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_126[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_64 (Multi  (None, 100, 1)      1401        ['layer_normalization_130[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_130[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_66 (Multi  (None, 100, 1)      1401        ['layer_normalization_134[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_134[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_68 (Multi  (None, 100, 1)      1401        ['layer_normalization_138[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_138[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_70 (Multi  (None, 100, 1)      1401        ['layer_normalization_142[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_142[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_72 (Multi  (None, 100, 1)      1401        ['layer_normalization_146[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_146[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 100, 1)      1401        ['layer_normalization_78[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_43 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_40[0][0]']\n",
            "                                                                                                  \n",
            " dropout_45 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_42[0][0]']\n",
            "                                                                                                  \n",
            " dropout_47 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_44[0][0]']\n",
            "                                                                                                  \n",
            " dropout_49 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_46[0][0]']\n",
            "                                                                                                  \n",
            " dropout_51 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_48[0][0]']\n",
            "                                                                                                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dropout_53 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_50[0][0]']\n",
            "                                                                                                  \n",
            " dropout_55 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_52[0][0]']\n",
            "                                                                                                  \n",
            " dropout_57 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_54[0][0]']\n",
            "                                                                                                  \n",
            " dropout_59 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_56[0][0]']\n",
            "                                                                                                  \n",
            " dropout_61 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_58[0][0]']\n",
            "                                                                                                  \n",
            " dropout_63 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_60[0][0]']\n",
            "                                                                                                  \n",
            " dropout_65 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_62[0][0]']\n",
            "                                                                                                  \n",
            " dropout_67 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_64[0][0]']\n",
            "                                                                                                  \n",
            " dropout_69 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_66[0][0]']\n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_68[0][0]']\n",
            "                                                                                                  \n",
            " dropout_73 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_70[0][0]']\n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_72[0][0]']\n",
            "                                                                                                  \n",
            " dropout_41 (Dropout)           (None, 100, 1)       0           ['multi_head_attention_38[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_80 (TFOpL  (None, 100, 1)      0           ['dropout_43[0][0]',             \n",
            " ambda)                                                           'input_20[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_84 (TFOpL  (None, 100, 1)      0           ['dropout_45[0][0]',             \n",
            " ambda)                                                           'input_21[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_88 (TFOpL  (None, 100, 1)      0           ['dropout_47[0][0]',             \n",
            " ambda)                                                           'input_22[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_92 (TFOpL  (None, 100, 1)      0           ['dropout_49[0][0]',             \n",
            " ambda)                                                           'input_23[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_96 (TFOpL  (None, 100, 1)      0           ['dropout_51[0][0]',             \n",
            " ambda)                                                           'input_24[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_100 (TFOp  (None, 100, 1)      0           ['dropout_53[0][0]',             \n",
            " Lambda)                                                          'input_25[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_104 (TFOp  (None, 100, 1)      0           ['dropout_55[0][0]',             \n",
            " Lambda)                                                          'input_26[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_108 (TFOp  (None, 100, 1)      0           ['dropout_57[0][0]',             \n",
            " Lambda)                                                          'input_27[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_112 (TFOp  (None, 100, 1)      0           ['dropout_59[0][0]',             \n",
            " Lambda)                                                          'input_28[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_116 (TFOp  (None, 100, 1)      0           ['dropout_61[0][0]',             \n",
            " Lambda)                                                          'input_29[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_120 (TFOp  (None, 100, 1)      0           ['dropout_63[0][0]',             \n",
            " Lambda)                                                          'input_30[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_124 (TFOp  (None, 100, 1)      0           ['dropout_65[0][0]',             \n",
            " Lambda)                                                          'input_31[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_128 (TFOp  (None, 100, 1)      0           ['dropout_67[0][0]',             \n",
            " Lambda)                                                          'input_32[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_132 (TFOp  (None, 100, 1)      0           ['dropout_69[0][0]',             \n",
            " Lambda)                                                          'input_33[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_136 (TFOp  (None, 100, 1)      0           ['dropout_71[0][0]',             \n",
            " Lambda)                                                          'input_34[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_140 (TFOp  (None, 100, 1)      0           ['dropout_73[0][0]',             \n",
            " Lambda)                                                          'input_35[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_144 (TFOp  (None, 100, 1)      0           ['dropout_75[0][0]',             \n",
            " Lambda)                                                          'input_36[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_76 (TFOpL  (None, 100, 1)      0           ['dropout_41[0][0]',             \n",
            " ambda)                                                           'input_19[0][0]']               \n",
            "                                                                                                  \n",
            " layer_normalization_83 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_80[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_87 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_84[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_91 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_88[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_95 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_92[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_99 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_96[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_103 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_100[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_107 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_104[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_111 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_108[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_115 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_112[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_119 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_116[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_123 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_120[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_127 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_124[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_131 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_128[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_135 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_132[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_139 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_136[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_143 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_140[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_147 (Layer  (None, 100, 1)      2           ['tf.__operators__.add_144[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_79 (LayerN  (None, 100, 1)      2           ['tf.__operators__.add_76[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_62 (Dense)               (None, 100, 100)     200         ['layer_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " dense_65 (Dense)               (None, 100, 100)     200         ['layer_normalization_87[0][0]'] \n",
            "                                                                                                  \n",
            " dense_68 (Dense)               (None, 100, 100)     200         ['layer_normalization_91[0][0]'] \n",
            "                                                                                                  \n",
            " dense_71 (Dense)               (None, 100, 100)     200         ['layer_normalization_95[0][0]'] \n",
            "                                                                                                  \n",
            " dense_74 (Dense)               (None, 100, 100)     200         ['layer_normalization_99[0][0]'] \n",
            "                                                                                                  \n",
            " dense_77 (Dense)               (None, 100, 100)     200         ['layer_normalization_103[0][0]']\n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 100, 100)     200         ['layer_normalization_107[0][0]']\n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 100, 100)     200         ['layer_normalization_111[0][0]']\n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 100, 100)     200         ['layer_normalization_115[0][0]']\n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 100, 100)     200         ['layer_normalization_119[0][0]']\n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 100, 100)     200         ['layer_normalization_123[0][0]']\n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 100, 100)     200         ['layer_normalization_127[0][0]']\n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 100, 100)     200         ['layer_normalization_131[0][0]']\n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 100, 100)     200         ['layer_normalization_135[0][0]']\n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 100, 100)     200         ['layer_normalization_139[0][0]']\n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 100, 100)     200         ['layer_normalization_143[0][0]']\n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 100, 100)     200         ['layer_normalization_147[0][0]']\n",
            "                                                                                                  \n",
            " dense_59 (Dense)               (None, 100, 100)     200         ['layer_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_81 (TFOpL  (None, 100, 100)    0           ['dense_62[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_80[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_85 (TFOpL  (None, 100, 100)    0           ['dense_65[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_84[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_89 (TFOpL  (None, 100, 100)    0           ['dense_68[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_88[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_93 (TFOpL  (None, 100, 100)    0           ['dense_71[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_92[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_97 (TFOpL  (None, 100, 100)    0           ['dense_74[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_96[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_101 (TFOp  (None, 100, 100)    0           ['dense_77[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_100[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_105 (TFOp  (None, 100, 100)    0           ['dense_80[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_104[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_109 (TFOp  (None, 100, 100)    0           ['dense_83[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_108[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_113 (TFOp  (None, 100, 100)    0           ['dense_86[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_112[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_117 (TFOp  (None, 100, 100)    0           ['dense_89[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_116[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_121 (TFOp  (None, 100, 100)    0           ['dense_92[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_120[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_125 (TFOp  (None, 100, 100)    0           ['dense_95[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_124[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_129 (TFOp  (None, 100, 100)    0           ['dense_98[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_128[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_133 (TFOp  (None, 100, 100)    0           ['dense_101[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_132[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_137 (TFOp  (None, 100, 100)    0           ['dense_104[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_136[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_141 (TFOp  (None, 100, 100)    0           ['dense_107[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_140[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_145 (TFOp  (None, 100, 100)    0           ['dense_110[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_144[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_77 (TFOpL  (None, 100, 100)    0           ['dense_59[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_76[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_81[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_88 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_85[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_92 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_89[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_96 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_93[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_100 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_97[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_104 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_101[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_108 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_105[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_112 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_109[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_116 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_113[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_120 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_117[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_124 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_121[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_128 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_125[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_132 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_129[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_136 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_133[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_140 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_137[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_144 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_141[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_148 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_145[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_80 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_77[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 100, 100)    80700       ['layer_normalization_84[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (Multi  (None, 100, 100)    80700       ['layer_normalization_88[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_88[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_45 (Multi  (None, 100, 100)    80700       ['layer_normalization_92[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_92[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_47 (Multi  (None, 100, 100)    80700       ['layer_normalization_96[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_96[0][0]'] \n",
            "                                                                                                  \n",
            " multi_head_attention_49 (Multi  (None, 100, 100)    80700       ['layer_normalization_100[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_100[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_51 (Multi  (None, 100, 100)    80700       ['layer_normalization_104[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_104[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_53 (Multi  (None, 100, 100)    80700       ['layer_normalization_108[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_108[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_55 (Multi  (None, 100, 100)    80700       ['layer_normalization_112[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_112[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_57 (Multi  (None, 100, 100)    80700       ['layer_normalization_116[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_116[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_59 (Multi  (None, 100, 100)    80700       ['layer_normalization_120[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_120[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_61 (Multi  (None, 100, 100)    80700       ['layer_normalization_124[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_124[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_63 (Multi  (None, 100, 100)    80700       ['layer_normalization_128[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_128[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_65 (Multi  (None, 100, 100)    80700       ['layer_normalization_132[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_132[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_67 (Multi  (None, 100, 100)    80700       ['layer_normalization_136[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_136[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_69 (Multi  (None, 100, 100)    80700       ['layer_normalization_140[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_140[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_71 (Multi  (None, 100, 100)    80700       ['layer_normalization_144[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_144[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_73 (Multi  (None, 100, 100)    80700       ['layer_normalization_148[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_148[0][0]']\n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 100, 100)    80700       ['layer_normalization_80[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_44 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_41[0][0]']\n",
            "                                                                                                  \n",
            " dropout_46 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_43[0][0]']\n",
            "                                                                                                  \n",
            " dropout_48 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_45[0][0]']\n",
            "                                                                                                  \n",
            " dropout_50 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_47[0][0]']\n",
            "                                                                                                  \n",
            " dropout_52 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_49[0][0]']\n",
            "                                                                                                  \n",
            " dropout_54 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_51[0][0]']\n",
            "                                                                                                  \n",
            " dropout_56 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_53[0][0]']\n",
            "                                                                                                  \n",
            " dropout_58 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_55[0][0]']\n",
            "                                                                                                  \n",
            " dropout_60 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_57[0][0]']\n",
            "                                                                                                  \n",
            " dropout_62 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_59[0][0]']\n",
            "                                                                                                  \n",
            " dropout_64 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_61[0][0]']\n",
            "                                                                                                  \n",
            " dropout_66 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_63[0][0]']\n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_65[0][0]']\n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_67[0][0]']\n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_69[0][0]']\n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_71[0][0]']\n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_73[0][0]']\n",
            "                                                                                                  \n",
            " dropout_42 (Dropout)           (None, 100, 100)     0           ['multi_head_attention_39[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_82 (TFOpL  (None, 100, 100)    0           ['dropout_44[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_81[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_86 (TFOpL  (None, 100, 100)    0           ['dropout_46[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_85[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_90 (TFOpL  (None, 100, 100)    0           ['dropout_48[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_89[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_94 (TFOpL  (None, 100, 100)    0           ['dropout_50[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_93[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_98 (TFOpL  (None, 100, 100)    0           ['dropout_52[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_97[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_102 (TFOp  (None, 100, 100)    0           ['dropout_54[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_101[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_106 (TFOp  (None, 100, 100)    0           ['dropout_56[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_105[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_110 (TFOp  (None, 100, 100)    0           ['dropout_58[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_109[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_114 (TFOp  (None, 100, 100)    0           ['dropout_60[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_113[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_118 (TFOp  (None, 100, 100)    0           ['dropout_62[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_117[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_122 (TFOp  (None, 100, 100)    0           ['dropout_64[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_121[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_126 (TFOp  (None, 100, 100)    0           ['dropout_66[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_125[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_130 (TFOp  (None, 100, 100)    0           ['dropout_68[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_129[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_134 (TFOp  (None, 100, 100)    0           ['dropout_70[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_133[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_138 (TFOp  (None, 100, 100)    0           ['dropout_72[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_137[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_142 (TFOp  (None, 100, 100)    0           ['dropout_74[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_141[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_146 (TFOp  (None, 100, 100)    0           ['dropout_76[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_145[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_78 (TFOpL  (None, 100, 100)    0           ['dropout_42[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_77[0][0]']\n",
            "                                                                                                  \n",
            " layer_normalization_85 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_82[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_89 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_86[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_93 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_90[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_97 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_94[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_101 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_98[0][0]']\n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " layer_normalization_105 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_102[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_109 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_106[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_113 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_110[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_117 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_114[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_121 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_118[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_125 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_122[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_129 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_126[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_133 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_130[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_137 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_134[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_141 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_138[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_145 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_142[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_149 (Layer  (None, 100, 100)    200         ['tf.__operators__.add_146[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_81 (LayerN  (None, 100, 100)    200         ['tf.__operators__.add_78[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_63 (Dense)               (None, 100, 100)     10100       ['layer_normalization_85[0][0]'] \n",
            "                                                                                                  \n",
            " dense_66 (Dense)               (None, 100, 100)     10100       ['layer_normalization_89[0][0]'] \n",
            "                                                                                                  \n",
            " dense_69 (Dense)               (None, 100, 100)     10100       ['layer_normalization_93[0][0]'] \n",
            "                                                                                                  \n",
            " dense_72 (Dense)               (None, 100, 100)     10100       ['layer_normalization_97[0][0]'] \n",
            "                                                                                                  \n",
            " dense_75 (Dense)               (None, 100, 100)     10100       ['layer_normalization_101[0][0]']\n",
            "                                                                                                  \n",
            " dense_78 (Dense)               (None, 100, 100)     10100       ['layer_normalization_105[0][0]']\n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 100, 100)     10100       ['layer_normalization_109[0][0]']\n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 100, 100)     10100       ['layer_normalization_113[0][0]']\n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 100, 100)     10100       ['layer_normalization_117[0][0]']\n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 100, 100)     10100       ['layer_normalization_121[0][0]']\n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 100, 100)     10100       ['layer_normalization_125[0][0]']\n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 100, 100)     10100       ['layer_normalization_129[0][0]']\n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 100, 100)     10100       ['layer_normalization_133[0][0]']\n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 100, 100)     10100       ['layer_normalization_137[0][0]']\n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 100, 100)     10100       ['layer_normalization_141[0][0]']\n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 100, 100)     10100       ['layer_normalization_145[0][0]']\n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 100, 100)     10100       ['layer_normalization_149[0][0]']\n",
            "                                                                                                  \n",
            " dense_60 (Dense)               (None, 100, 100)     10100       ['layer_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_83 (TFOpL  (None, 100, 100)    0           ['dense_63[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_82[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_87 (TFOpL  (None, 100, 100)    0           ['dense_66[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_86[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_91 (TFOpL  (None, 100, 100)    0           ['dense_69[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_90[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_95 (TFOpL  (None, 100, 100)    0           ['dense_72[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_94[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_99 (TFOpL  (None, 100, 100)    0           ['dense_75[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_98[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_103 (TFOp  (None, 100, 100)    0           ['dense_78[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_102[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_107 (TFOp  (None, 100, 100)    0           ['dense_81[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_106[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_111 (TFOp  (None, 100, 100)    0           ['dense_84[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_110[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_115 (TFOp  (None, 100, 100)    0           ['dense_87[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_114[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_119 (TFOp  (None, 100, 100)    0           ['dense_90[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_118[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_123 (TFOp  (None, 100, 100)    0           ['dense_93[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_122[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_127 (TFOp  (None, 100, 100)    0           ['dense_96[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_126[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_131 (TFOp  (None, 100, 100)    0           ['dense_99[0][0]',               \n",
            " Lambda)                                                          'tf.__operators__.add_130[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_135 (TFOp  (None, 100, 100)    0           ['dense_102[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_134[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_139 (TFOp  (None, 100, 100)    0           ['dense_105[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_138[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_143 (TFOp  (None, 100, 100)    0           ['dense_108[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_142[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_147 (TFOp  (None, 100, 100)    0           ['dense_111[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_146[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_79 (TFOpL  (None, 100, 100)    0           ['dense_60[0][0]',               \n",
            " ambda)                                                           'tf.__operators__.add_78[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d_20 (G  (None, 100)         0           ['tf.__operators__.add_83[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_21 (G  (None, 100)         0           ['tf.__operators__.add_87[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_22 (G  (None, 100)         0           ['tf.__operators__.add_91[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_23 (G  (None, 100)         0           ['tf.__operators__.add_95[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_24 (G  (None, 100)         0           ['tf.__operators__.add_99[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_25 (G  (None, 100)         0           ['tf.__operators__.add_103[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_26 (G  (None, 100)         0           ['tf.__operators__.add_107[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_27 (G  (None, 100)         0           ['tf.__operators__.add_111[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_28 (G  (None, 100)         0           ['tf.__operators__.add_115[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_29 (G  (None, 100)         0           ['tf.__operators__.add_119[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_30 (G  (None, 100)         0           ['tf.__operators__.add_123[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_31 (G  (None, 100)         0           ['tf.__operators__.add_127[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_32 (G  (None, 100)         0           ['tf.__operators__.add_131[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_33 (G  (None, 100)         0           ['tf.__operators__.add_135[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_34 (G  (None, 100)         0           ['tf.__operators__.add_139[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_35 (G  (None, 100)         0           ['tf.__operators__.add_143[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_36 (G  (None, 100)         0           ['tf.__operators__.add_147[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_19 (G  (None, 100)         0           ['tf.__operators__.add_79[0][0]']\n",
            " lobalAveragePooling1D)                                                                           \n",
            "                                                                                                  \n",
            " dense_64 (Dense)               (None, 10)           1010        ['global_average_pooling1d_20[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_67 (Dense)               (None, 10)           1010        ['global_average_pooling1d_21[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_70 (Dense)               (None, 10)           1010        ['global_average_pooling1d_22[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_73 (Dense)               (None, 10)           1010        ['global_average_pooling1d_23[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_76 (Dense)               (None, 10)           1010        ['global_average_pooling1d_24[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_79 (Dense)               (None, 10)           1010        ['global_average_pooling1d_25[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 10)           1010        ['global_average_pooling1d_26[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 10)           1010        ['global_average_pooling1d_27[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 10)           1010        ['global_average_pooling1d_28[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 10)           1010        ['global_average_pooling1d_29[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 10)           1010        ['global_average_pooling1d_30[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 10)           1010        ['global_average_pooling1d_31[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 10)           1010        ['global_average_pooling1d_32[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 10)           1010        ['global_average_pooling1d_33[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 10)           1010        ['global_average_pooling1d_34[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 10)           1010        ['global_average_pooling1d_35[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 10)           1010        ['global_average_pooling1d_36[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_61 (Dense)               (None, 10)           1010        ['global_average_pooling1d_19[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " tf.math.add_17 (TFOpLambda)    (None, 10)           0           ['dense_64[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_18 (TFOpLambda)    (None, 10)           0           ['dense_67[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_19 (TFOpLambda)    (None, 10)           0           ['dense_70[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_20 (TFOpLambda)    (None, 10)           0           ['dense_73[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_21 (TFOpLambda)    (None, 10)           0           ['dense_76[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_22 (TFOpLambda)    (None, 10)           0           ['dense_79[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_23 (TFOpLambda)    (None, 10)           0           ['dense_82[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_24 (TFOpLambda)    (None, 10)           0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_25 (TFOpLambda)    (None, 10)           0           ['dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_26 (TFOpLambda)    (None, 10)           0           ['dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_27 (TFOpLambda)    (None, 10)           0           ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_28 (TFOpLambda)    (None, 10)           0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.add_29 (TFOpLambda)    (None, 10)           0           ['dense_100[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.add_30 (TFOpLambda)    (None, 10)           0           ['dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.add_31 (TFOpLambda)    (None, 10)           0           ['dense_106[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.add_32 (TFOpLambda)    (None, 10)           0           ['dense_109[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.add_33 (TFOpLambda)    (None, 10)           0           ['dense_112[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 180)          0           ['dense_61[0][0]',               \n",
            "                                                                  'tf.math.add_17[0][0]',         \n",
            "                                                                  'tf.math.add_18[0][0]',         \n",
            "                                                                  'tf.math.add_19[0][0]',         \n",
            "                                                                  'tf.math.add_20[0][0]',         \n",
            "                                                                  'tf.math.add_21[0][0]',         \n",
            "                                                                  'tf.math.add_22[0][0]',         \n",
            "                                                                  'tf.math.add_23[0][0]',         \n",
            "                                                                  'tf.math.add_24[0][0]',         \n",
            "                                                                  'tf.math.add_25[0][0]',         \n",
            "                                                                  'tf.math.add_26[0][0]',         \n",
            "                                                                  'tf.math.add_27[0][0]',         \n",
            "                                                                  'tf.math.add_28[0][0]',         \n",
            "                                                                  'tf.math.add_29[0][0]',         \n",
            "                                                                  'tf.math.add_30[0][0]',         \n",
            "                                                                  'tf.math.add_31[0][0]',         \n",
            "                                                                  'tf.math.add_32[0][0]',         \n",
            "                                                                  'tf.math.add_33[0][0]']         \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLambda)  (None, 180, 1)       0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_150 (Layer  (None, 180, 1)      2           ['tf.expand_dims_1[0][0]']       \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " multi_head_attention_74 (Multi  (None, 180, 1)      2521        ['layer_normalization_150[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_150[0][0]']\n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 180, 1)       0           ['multi_head_attention_74[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_148 (TFOp  (None, 180, 1)      0           ['dropout_77[0][0]',             \n",
            " Lambda)                                                          'tf.expand_dims_1[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_151 (Layer  (None, 180, 1)      2           ['tf.__operators__.add_148[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 180, 180)     360         ['layer_normalization_151[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_149 (TFOp  (None, 180, 180)    0           ['dense_113[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_148[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_152 (Layer  (None, 180, 180)    360         ['tf.__operators__.add_149[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " multi_head_attention_75 (Multi  (None, 180, 180)    260460      ['layer_normalization_152[0][0]',\n",
            " HeadAttention)                                                   'layer_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 180, 180)     0           ['multi_head_attention_75[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_150 (TFOp  (None, 180, 180)    0           ['dropout_78[0][0]',             \n",
            " Lambda)                                                          'tf.__operators__.add_149[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " layer_normalization_153 (Layer  (None, 180, 180)    360         ['tf.__operators__.add_150[0][0]'\n",
            " Normalization)                                                  ]                                \n",
            "                                                                                                  \n",
            " dense_114 (Dense)              (None, 180, 180)     32580       ['layer_normalization_153[0][0]']\n",
            "                                                                                                  \n",
            " tf.__operators__.add_151 (TFOp  (None, 180, 180)    0           ['dense_114[0][0]',              \n",
            " Lambda)                                                          'tf.__operators__.add_150[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " global_average_pooling1d_37 (G  (None, 180)         0           ['tf.__operators__.add_151[0][0]'\n",
            " lobalAveragePooling1D)                                          ]                                \n",
            "                                                                                                  \n",
            " dropout_79 (Dropout)           (None, 180)          0           ['global_average_pooling1d_37[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_115 (Dense)              (None, 100)          18100       ['dropout_79[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)           (None, 100)          0           ['dense_115[0][0]']              \n",
            "                                                                                                  \n",
            " dense_116 (Dense)              (None, 20)           2020        ['dropout_80[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)           (None, 20)           0           ['dense_116[0][0]']              \n",
            "                                                                                                  \n",
            " dense_117 (Dense)              (None, 5)            105         ['dropout_81[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,005,540\n",
            "Trainable params: 2,005,540\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "fold 1\n",
            "Y_train\n",
            "58642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "534/534 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.4612\n",
            "Epoch 1: val_accuracy improved from -inf to 0.41246, saving model to output/train_severity_12_28/08_38/weights_1.hdf5\n",
            "534/534 [==============================] - 667s 1s/step - loss: 0.4214 - accuracy: 0.4612 - val_loss: 0.4614 - val_accuracy: 0.4125\n",
            "Epoch 2/100\n",
            "534/534 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.6042\n",
            "Epoch 2: val_accuracy improved from 0.41246 to 0.51288, saving model to output/train_severity_12_28/08_38/weights_1.hdf5\n",
            "534/534 [==============================] - 677s 1s/step - loss: 0.3401 - accuracy: 0.6042 - val_loss: 0.4047 - val_accuracy: 0.5129\n",
            "Epoch 3/100\n",
            "534/534 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.6900\n",
            "Epoch 3: val_accuracy improved from 0.51288 to 0.55544, saving model to output/train_severity_12_28/08_38/weights_1.hdf5\n",
            "534/534 [==============================] - 665s 1s/step - loss: 0.2864 - accuracy: 0.6900 - val_loss: 0.3917 - val_accuracy: 0.5554\n",
            "Epoch 4/100\n",
            "534/534 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.7507\n",
            "Epoch 4: val_accuracy improved from 0.55544 to 0.59373, saving model to output/train_severity_12_28/08_38/weights_1.hdf5\n",
            "534/534 [==============================] - 662s 1s/step - loss: 0.2444 - accuracy: 0.7507 - val_loss: 0.3561 - val_accuracy: 0.5937\n",
            "Epoch 5/100\n",
            "445/534 [========================>.....] - ETA: 1:45 - loss: 0.2165 - accuracy: 0.7861"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "xrhsjgLr47SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "27hVEYRz4-m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "JpbGYt8p4_4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "IR3ZGYgF5B-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "8fbxpHFf5DNu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4oGRv4Q48nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}